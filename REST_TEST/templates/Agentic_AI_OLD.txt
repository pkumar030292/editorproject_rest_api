from llama_cpp import Llama
import json
import os

# ---------------------------
# Configure your model path
# ---------------------------
MODEL_PATH = "models/capybarahermes-2.5-mistral-7b.Q2_K.gguf"

# Initialize the LLaMA model
llm = Llama(
    model_path=MODEL_PATH,
    n_ctx=2048,
    n_threads=4  # Adjust based on your CPU
)

# ---------------------------
# Define a goal
# ---------------------------
goal = "Design a basic offline Python project for managing expenses"

# Pre-defined reasoning steps
steps = [
    "Understand and rephrase the goal",
    "Break down the goal into actionable tasks",
    "Identify tools and Python libraries needed",
    "Create a project folder structure",
    "Generate a sample code outline",
    "Summarize the full plan"
]

memory = []  # Agent memory to hold step results

# ---------------------------
# Main Agent Loop
# ---------------------------
for i, step in enumerate(steps):
    prompt = f"""
You are an intelligent AI agent working offline.
Your goal is: "{goal}"

Memory of previous steps:
{json.dumps(memory, indent=2)}

Current step ({i + 1}/{len(steps)}): {step}
Respond clearly and concisely.
"""

    print(f"\nüîπ Step {i + 1}: {step}")
    result = llm(prompt, max_tokens=512)
    output = result['choices'][0]['text'].strip()

    memory.append({
        "step": step,
        "output": output
    })

    print(f"‚úÖ Output:\n{output}")

# ---------------------------
# Save results to file
# ---------------------------
save_path = "agentic_ai_memory.json"
with open(save_path, "w", encoding="utf-8") as f:
    json.dump(memory, f, indent=2, ensure_ascii=False)

print(f"\nüìÅ Agent memory saved to: {save_path}")
